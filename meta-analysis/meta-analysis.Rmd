---
title: "Intraoperative Margin Assessment Systematic Review and Meta-analysis"
author: "Jim Higginson"
date: "04/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mvtnorm)
library(ellipse)
library(mvmeta)
library(meta)
library(ggthemes)
library(xtable) # allows export to latex

```

## Data input

Getting the data in

```{r input data}

drop_empty <- function(x) {!all(is.na(x))}
    # function to be used below to drop any columns that are totally empty.
cols_to_keep <- c(
  "Covidence #",
  "Study ID",
  "Title",
  "First Author Name",
  "Year of publication",
  "Number of patients included",
  "modality_group",
  "frozen_group",
#   muting these columns whilst I work out the pivoting to keep it cleaner
  "Number of margins included",
  "Study design",
  "HNSCC sub-sites included in study",
#  "HPV status of participants",
  "Mean age",
  "Population description",
#  "What intraoperative margin tool is being evaluated in this study?",
  "Modality Diagnostic tool 1",
#  "Modality Diagnostic tool 2",
  "True negatives Diagnostic tool 1",
  "True positives Diagnostic tool 1",
  "False negatives Diagnostic tool 1",
  "False positives Diagnostic tool 1"
)


meta.data <- read_csv('2022-06-13_final_consensus_data.csv') %>% 
# This produces a very wide table, with repeated column titles for four diagnostic modalitiies, even though most papers will only discuss one. 
  select_if(drop_empty) %>% 
   filter(`Reviewer Name` == 'Consensus') %>% 
   select(cols_to_keep) %>% 
   rename(
   "TN" = "True negatives Diagnostic tool 1",
   "TP" = "True positives Diagnostic tool 1",
   "FN" = "False negatives Diagnostic tool 1",
   "FP" = "False positives Diagnostic tool 1"
   ) %>% 
   drop_na(TP)



```

## Univariate analysis

Hutan recommended using two or three different R packages to evaluate the results to ensure that they are believable.
I'm running this practice based on (Shim et al 2019 tutorial)[https://doi.org/10.4178/epih.e2019007]

```{r}
sensitivity.logit <- metaprop(
  meta.data$TP, 
  meta.data$TP + meta.data$FN,
  fixed=FALSE,
  random=TRUE,
  sm = 'PLOGIT',
  method.ci = 'CP',
  studlab = meta.data$`Study ID`,
  subgroup = meta.data$`modality_group`
)

specificity.logit <- metaprop(
  meta.data$TN,
  meta.data$TN + meta.data$FP,
  fixed=FALSE,
  random=TRUE,
  sm = 'PLOGIT',
  method.ci = 'CP',
  studlab = meta.data$`Study ID`,
  subgroup = meta.data$`modality_group`
)

print(sensitivity.logit, digits=3)
print(specificity.logit, digits=3)
```


## forest plot


```{r sensitivity plot, echo=FALSE}
pdf('sensitivity_forest.pdf', width = 8, height = 14)
forest(
  sensitivity.logit,
  digits = 3,
  rightcols = c('effect','ci'),
  rightlabs = c('Sensitivity','95% CI'),
  leftcols = c('studlab','n'),
  leftlabs = c('Study', 'n'),
  print.subgroup.name = FALSE,
  pooled.events = FALSE,
  colgap.forest.left = '2cm',
  addrows.below.overall = 2
)
dev.off()

```

```{r specificity plot, echo=FALSE}
pdf('specificity_forest.pdf', width = 8, height = 14)
forest(
  specificity.logit,
   digits = 3,
  rightcols = c('effect','ci'),
  rightlabs = c('Specificity','95% CI'),
  leftcols = c('studlab','n'),
  leftlabs = c('Study', 'n'),
  print.subgroup.name = FALSE,
  pooled.events = FALSE,
  colgap.forest.left = '2cm',
  addrows.below.overall = 2
)
dev.off()
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


# Diagnostic odds ratio

```{r}
DOR_model <- metabin(
  TP,
  TP+FP,
  FN,
  FN+TN, 
  sm='DOR', 
  comb.fixed=FALSE,
  comb.random=TRUE, 
  method='Inverse', 
  studlab = `Study ID`, 
  byvar=modality_group, 
  data=meta.data)
```

```{r}
pdf('dor_forest.pdf', width = 10, height = 14)
forest(
  DOR_model,
   digits = 1,
   rightcols = c('effect','ci'),
   rightlabs = c('95% CI'),
   leftcols = c('studlab'),
   leftlabs = c('Study'),
  print.subgroup.name = FALSE,
  pooled.events = FALSE,
  colgap.forest.left = '2cm',
  addrows.below.overall = 2
)
dev.off()
```


```{r bivariate analysis}
detach(package:meta)
library(mada) 
#needs to be loaded after detaching meta as they clash apparently (see Shim2019)
```

Now generate the same forest plots using `mada` instead of `meta` - the only difference being that they don't show the overall effec size of the summary stats.

```{r}
forest(madad(meta.data), type='sens', xlab='Sensitivity', snames=meta.data$`Study ID`)
forest(madad(meta.data), type='spec', xlab='Specificity', snames=meta.data$`Study ID`)
forest(madauni(meta.data))
```

Now create the bivariate model with the `reitsma' function
```{r}
fit <- reitsma(meta.data, correction.control='all')
plot(fit, sroclwd = 2, xlim = c(0,1), ylim = c(0,1), main = "SROC curve (bivariate model) for Diagnostic Test Accuracy")
points(fpr(meta.data), sens(meta.data), pch=20)
legend("bottomright", c("data", "summary estimate", "AUC=?", "DOR=?", "Sensitivity=?", "Specificity=?"), pch = c(20,1,1000,1000,1000,1000) ) 
legend("bottomleft", c("SROC", "95% CI region"), lwd = c(2,1))
```

Now I'm going to try and plot it nicely in ggplot

```{r}

confidence_region <- as_tibble(ROCellipse(fit)$ROCellipse)
summary_data <- as_tibble(ROCellipse(fit)$fprsens)
sroc_curve <- as_tibble(sroc(fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better

meta.data <- meta.data %>% 
  mutate(
    fpr = fpr(meta.data),
    sens = sens(meta.data)
  )

ggplot(data = meta.data) +
  geom_line(data = sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr, sens, size = `Number of margins included`), shape = 1, alpha = 0.5) +
  geom_point(data = summary_data, aes(V1, V2), size = 3) +
  labs(title = 'Summary ROC curve for all studies') +
  xlab('False positive rate') +
  ylab('Sensitivity') + 
  annotate('text', x = 0.8, y = 0, label = str_interp("AUC = $[.3f]{AUC(fit)$AUC}")) +
  annotate('text', x = 0.8, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - summary_data$V1}")) +
  annotate('text', x = 0.8, y = 0.2, label = str_interp("Sensitivity = $[.3f]{summary_data$V2}")) +
  theme_clean()


```

```{r}
stain.data <- meta.data %>% 
  filter(modality_group == 'Topical Staining')
stain.fit <- reitsma(stain.data, correction.control='all')
stain.confidence_region <- as_tibble(ROCellipse(stain.fit)$ROCellipse)
stain.summary_data <- as_tibble(ROCellipse(stain.fit)$fprsens)
stain.sroc_curve <- as_tibble(sroc(stain.fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1)
stain.auc = AUC(stain.fit)$AUC


frozen.data<- meta.data %>% 
  filter(modality_group == 'Frozen Section')
frozen.fit <- reitsma(frozen.data, correction.control='all')
frozen.confidence_region <- as_tibble(ROCellipse(frozen.fit)$ROCellipse)
frozen.summary_data <- as_tibble(ROCellipse(frozen.fit)$fprsens)
frozen.sroc_curve <- as_tibble(sroc(frozen.fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better
frozen.auc = AUC(frozen.fit)$AUC

optical.data<- meta.data %>% 
  filter(modality_group == 'Optical Techniques')
optical.fit <- reitsma(optical.data, correction.control='all')
optical.confidence_region <- as_tibble(ROCellipse(optical.fit)$ROCellipse)
optical.summary_data <- as_tibble(ROCellipse(optical.fit)$fprsens)
optical.sroc_curve <- as_tibble(sroc(optical.fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better
optical.auc = AUC(optical.fit)$AUC

chemo.data<- meta.data %>% 
  filter(modality_group == 'Chemiluminescence')
chemo.fit <- reitsma(chemo.data, correction.control='all')
chemo.confidence_region <- as_tibble(ROCellipse(chemo.fit)$ROCellipse)
chemo.summary_data <- as_tibble(ROCellipse(chemo.fit)$fprsens)
chemo.sroc_curve <- as_tibble(sroc(chemo.fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better
chemo.auc = AUC(chemo.fit)$AUC

tic.data<- meta.data %>% 
  filter(modality_group == 'Touch Imprint Cytology')
tic.fit <- reitsma(tic.data, correction.control='all')
tic.confidence_region <- as_tibble(ROCellipse(tic.fit)$ROCellipse)
tic.summary_data <- as_tibble(ROCellipse(tic.fit)$fprsens)
tic.sroc_curve <- as_tibble(sroc(tic.fit)) %>% 
  add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better
tic.auc = AUC(tic.fit)$AUC

# auto.data<- meta.data %>% 
#   filter(modality_group == 'Autofluorescence')
# auto.fit <- reitsma(auto.data, correction.control='all')
# auto.confidence_region <- as_tibble(ROCellipse(auto.fit)$ROCellipse)
# auto.summary_data <- as_tibble(ROCellipse(auto.fit)$fprsens)
# auto.sroc_curve <- as_tibble(sroc(auto.fit)) %>% 
#   add_row(fpr = 0, V2 = 0, .before = 1) # this line adds a 0,0 point on the line to make the graph look better
# auto.auc = AUC(auto.fit)$AUC
```

```{r}
ggplot() +
  geom_line(data = stain.sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = stain.confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr(stain.data), sens(stain.data)), size = 1) +
  geom_point(data = stain.summary_data, aes(V1, V2), shape = 21) +
  labs(title = 'Summary ROC curve for stain studies') +
  xlab('False positive rate') +
  ylab('Sensitivity') +
  annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{stain.auc}")) +
   annotate('text', x = 0.9, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - stain.summary_data$V1}")) +
  annotate('text', x = 0.9, y = 0.2, label = str_interp("Sensitivity = $[.3f]{stain.summary_data$V2}")) +
  theme_classic()
ggsave('stain_sroc.png')
```
```{r}
ggplot() +
  geom_line(data = frozen.sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = frozen.confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr(frozen.data), sens(frozen.data))) +
  geom_point(data = frozen.summary_data, aes(V1, V2)) +
  labs(title = 'Summary ROC curve for frozen studies') +
  xlab('False positive rate') +
  ylab('Sensitivity')+
  annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{frozen.auc}")) +
   annotate('text', x = 0.9, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - frozen.summary_data$V1}")) +
  annotate('text', x = 0.9, y = 0.2, label = str_interp("Sensitivity = $[.3f]{frozen.summary_data$V2}")) +
  theme_classic()
ggsave('frozen_sroc.png')
```

```{r}
ggplot() +
  geom_line(data = chemo.sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = chemo.confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr(chemo.data), sens(chemo.data)), size = 1) +
  geom_point(data = chemo.summary_data, aes(V1, V2), shape = 21) +
  labs(title = 'Summary ROC curve for chemo studies') +
  xlab('False positive rate') +
  ylab('Sensitivity')+
  annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{chemo.auc}")) +
  annotate('text', x = 0.9, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - chemo.summary_data$V1}")) +
  annotate('text', x = 0.9, y = 0.2, label = str_interp("Sensitivity = $[.3f]{chemo.summary_data$V2}")) +
  theme_classic()
ggsave('chemo_sroc.png')
```

```{r}
ggplot() +
  geom_line(data = tic.sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = tic.confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr(tic.data), sens(tic.data)), size = 1) +
  geom_point(data = tic.summary_data, aes(V1, V2), shape = 21) +
  labs(title = 'Summary ROC curve for tic studies') +
  xlab('False positive rate') +
  ylab('Sensitivity')+
  annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{tic.auc}")) +
   annotate('text', x = 0.9, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - tic.summary_data$V1}")) +
  annotate('text', x = 0.9, y = 0.2, label = str_interp("Sensitivity = $[.3f]{tic.summary_data$V2}")) +
  theme_classic()
ggsave('tic_sroc.png')
```

```{r}
ggplot() +
  geom_line(data = optical.sroc_curve, aes(fpr, V2)) +
  geom_polygon(data = optical.confidence_region, aes(V1, V2), alpha = 0.3) +
  geom_point(aes(fpr(optical.data), sens(optical.data)), size = 1) +
  geom_point(data = optical.summary_data, aes(V1, V2), shape = 21) +
  labs(title = 'Summary ROC curve for optical studies') +
  xlab('False positive rate') +
  ylab('Sensitivity')+
  annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{optical.auc}")) +
  annotate('text', x = 0.9, y = 0.1, label = str_interp("Specificity = $[.3f]{1 - optical.summary_data$V1}")) +
  annotate('text', x = 0.9, y = 0.2, label = str_interp("Sensitivity = $[.3f]{optical.summary_data$V2}")) +
  theme_classic()
ggsave('optical_sroc.png')
```
 
```{r}
# ggplot() +
#   geom_line(data = auto.sroc_curve, aes(fpr, V2)) +
#   geom_polygon(data = auto.confidence_region, aes(V1, V2), alpha = 0.3) +
#   geom_point(aes(fpr(auto.data), sens(auto.data)), size = 1) +
#   geom_point(data = auto.summary_data, aes(V1, V2), shape = 21) +
#   labs(title = 'Summary ROC curve for auto studies') +
#   xlab('False positive rate') +
#   ylab('Sensitivity') +
#   annotate('text', x = 0.9, y = 0, label = str_interp("AUC = $[.3f]{auto.auc}")) +
#   theme_classic()
# ggsave('auto_sroc.png')
```


Now to do the analysis separating specimen from defect driven frozen section.


```{r}
library(meta)
#reload meta package here for metaprop for analysis of defect v specimen driven frozen section

frozen.data <- frozen.data %>% 
  filter(frozen_group == 'Specimen-driven' | frozen_group == 'Defect-driven')

frozen.sens.logit <- metaprop(
  frozen.data$TP, 
  frozen.data$TP + frozen.data$FN,
  fixed=FALSE,
  random=TRUE,
  sm = 'PLOGIT',
  method.ci = 'CP',
  studlab = frozen.data$`Study ID`,
  subgroup = frozen.data$`frozen_group`
)

frozen.spec.logit <- metaprop(
  frozen.data$TN,
  frozen.data$TN + frozen.data$FP,
  fixed=FALSE,
  random=TRUE,
  sm = 'PLOGIT',
  method.ci = 'CP',
  studlab = frozen.data$`Study ID`,
  subgroup = frozen.data$`frozen_group`
)
```

```{r}

pdf('frozen_sensitivity_forest.pdf', width = 8, height = 6)
forest(
  frozen.sens.logit,
  digits = 3,
  rightcols = c('effect','ci'),
  rightlabs = c('Sensitivity','95% CI'),
  leftcols = c('studlab','n'),
  leftlabs = c('Study', 'n'),
  print.subgroup.name = FALSE,
  pooled.events = FALSE,
  colgap.forest.left = '2cm',
  addrows.below.overall = 2
)
dev.off()

```

```{r}
pdf('frozen_specificity_forest.pdf', width = 8, height = 6)
forest(
  frozen.spec.logit,
  digits = 3,
  rightcols = c('effect','ci'),
  rightlabs = c('Specificity','95% CI'),
  leftcols = c('studlab','n'),
  leftlabs = c('Study', 'n'),
  print.subgroup.name = FALSE,
  pooled.events = FALSE,
  colgap.forest.left = '2cm',
  addrows.below.overall = 2
)
dev.off()
```
 
 
 
 
 # Heterogeneity analysis here.





```{r}
diagnostic_heterogeneity <- meta.data %>% # creates a dataframe suitable for logit transformation
  mutate(
    FN = if_else(FN == 0, 0.001, FN),
    FP = if_else(FP == 0, 0.001, FP),
    sensitivity = TP/(TP+FN),
    specificity = TN/(TN+FP),
    logit_sn = sensitivity/(1-sensitivity),
    logit_sp = specificity/(1-specificity)
  )

correlation = cor(diagnostic_heterogeneity$logit_sn, diagnostic_heterogeneity$logit_sp)
```



Now the final metaregression analysis

```{r}

diagnostic_metaregression <- metareg(DOR_model, modality_group, method.tau = 'REML', digits = 3)
```

# publication bias analysis

Overall funnel plot
```{r}

funnel(DOR_model)

```

This basic plot is pretty ugly. Took me a while to work out how to get the DOR data out: the effect size in the model is accessible via `DOR_model$TE`, but this has been log transformed. To reverse it and get the plottable data out: `exp(DOR_model$TE`)

<<<<<<< HEAD

```{r funnel plot}
DOR_model$data %>% 
  mutate(
    DOR = exp(DOR_model$TE),
    precision = DOR_model$seTE ^ -1
  ) %>% 
  ggplot() +
  geom_point(aes(DOR, precision, shape = modality_group, colour = modality_group)) +
  geom_vline(xintercept = exp(DOR_model$TE.random), linetype='dashed') +
  geom_segment(aes(
    x = exp(DOR_model$lower.predict), 
    y = 0, 
    xend = exp(DOR_model$TE.random), 
    yend = 4
    ), linetype='dotted') +
    geom_segment(aes(
    x = exp(DOR_model$upper.predict), 
    y = 0, 
    xend = exp(DOR_model$TE.random), 
    yend = 4
    ), linetype='dotted') +
  scale_x_log10() +
  scale_colour_colorblind() +
  theme_classic()  
ggsave('funnel_plot.png')
```
This is similar to, but in my view better than, the inbuilt funnel, the main difference is the latter uses 'effective study size' (inverted) rather than precision:

```{r}
funnel.meta(DOR_model
            )
```

Now, to quantify this, can use the duval and tweedie trimfill method as recommended by Burkner and Doebler.

Note that the heterogeneity of the `DOR_model` is very high (`r DOR_model$I2`), so it may be worth trimming out the outliers at a second run as per https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html, section 9.2.1.4.

```{r}
tf <- trimfill(DOR_model)
```

and plotted with the imputed `r tf$k0` studies:

```{r}
funnel.meta(tf)
```

Now doing the trimfill funnel myself using ggplot
```{r trimfill funnel}
tf.DOR = exp(tf$TE)
tf.precision = tf$seTE ^ -1
  
  ggplot() +
  geom_point(
    aes(x = tf.DOR, 
        y = tf.precision, 
        shape = startsWith(tf$studlab,'Filled'),
        colour = startsWith(tf$studlab,'Filled')
        )
    ) +
  geom_vline(xintercept = exp(tf$TE.random), linetype='dashed') +
  geom_segment(aes(
    x = exp(tf$lower.predict), 
    y = 0, 
    xend = exp(tf$TE.random), 
    yend = 4
    ), linetype='dotted') +
    geom_segment(aes(
    x = exp(tf$upper.predict), 
    y = 0, 
    xend = exp(tf$TE.random), 
    yend = 4
    ), linetype='dotted') +
  scale_x_log10() +
  labs(shape = '', colour = '') +
  scale_shape_discrete(labels = c('Real studies', 'Predicted studies')) +
  scale_colour_colorblind(labels = c('Real studies', 'Predicted studies')) +
  theme_classic() +
  xlab('Diagnostic Odds Ratio') +
  ylab('Study precision')  # note that precision is 1/standard error here - add to caption.
ggsave('trim_filled_funnel_plot.png')
```




Now funnel plot for just frozen section

```{r}
frozen.DOR_model <- metabin(
  TP,
  TP+FP,
  FN,
  FN+TN, 
  sm='DOR', 
  comb.fixed=FALSE,
  comb.random=TRUE, 
  method='Inverse', 
  studlab = `Study ID`, 
  data=frozen.data)
```

This chunk will produce a nice table to export for the manuscript.

```{r}
export.meta.data <- meta.data %>% 
  select(
    !c(
      `Covidence #`,
      `Title`,
      `First Author Name`,
      `Year of publication`,
      `frozen_group`,
      `fpr`,
      `sens`
    )
  )

export.table <- xtable(
  export.meta.data,
  caption = c('short caption', 'full caption'),
  label = 'tab:study_details',
  booktabs=TRUE
)

print(export.table, size="\\tiny", file = '../manuscript/tables/study_details.tex')
```


# Now qualitative data

```{r}

qual_cols = c(
"Study ID",
"Was a consecutive or random sample of patients enrolled?",
"Was a consecutive or random sample of patients enrolled? supporting text",
"Was a case-control design avoided?",
"Was a case-control design avoided? supporting text",
"Did the study avoid inappropriate exclusions?",
"Did the study avoid inappropriate exclusions? supporting text",
"Could the selection of patients have introduced bias?",
"Could the selection of patients have introduced bias? supporting text",
"Is there concern that the included patients do not match the review question?",
"Is there concern that the included patients do not match the review question? supporting text",
"Were the index test results interpreted without knowledge of the results of the reference standard?",
"Were the index test results interpreted without knowledge of the results of the reference standard? supporting text",
"If a threshold was used, was it pre-specified?",
"If a threshold was used, was it pre-specified? supporting text",
"Could the conduct or interpretation of the index test have introduced bias?",
"Could the conduct or interpretation of the index test have introduced bias? supporting text",
"Is there concern that the index test, its conduct, or interpretation differ from the review question?",
"Is there concern that the index test, its conduct, or interpretation differ from the review question? supporting text",
"Is the reference standard likely to correctly classify the target condition?",
"Is the reference standard likely to correctly classify the target condition? supporting text",
"Were the reference standard results interpreted without knowledge of the results of the index test?",
"Were the reference standard results interpreted without knowledge of the results of the index test? supporting text",
"Could the reference standard, its conduct, or its interpretation have introduced bias?",
"Could the reference standard, its conduct, or its interpretation have introduced bias? supporting text",
"Is there concern that the target condition as defined by the reference standard does not match the review question?",
"Is there concern that the target condition as defined by the reference standard does not match the review question? supporting text",
"Was there an appropriate interval between index test(s) and reference standard?",
"Was there an appropriate interval between index test(s) and reference standard? supporting text",
"Did all patients have reference histopathology?",
"Did all patients have reference histopathology? supporting text",
"Were all patients included in the analysis?",
"Were all patients included in the analysis? supporting text",
"Could the patient flow have introduced bias?",
"Could the patient flow have introduced bias? supporting text",
"What SORT score is this ?",
"What SORT score is this ? supporting text"                                                                              
)

qual.data <-  read_csv('all_data_unverified_for_testing.csv') %>% 
# This produces a table of all the quality data
  filter(`Reviewer Name` == 'James Higginson') %>% 
  select(qual_cols)

quadas.qs <- c(
"Study ID",
"Could the selection of patients have introduced bias?",
"Is there concern that the included patients do not match the review question?",
"Could the conduct or interpretation of the index test have introduced bias?",
"Is there concern that the index test, its conduct, or interpretation differ from the review question?",
"Could the reference standard, its conduct, or its interpretation have introduced bias?",
"Is there concern that the target condition as defined by the reference standard does not match the review question?",
"Could the patient flow have introduced bias?",
"What SORT score is this ?"
)
final.qual <- qual.data %>% 
  select(quadas.qs)

export.qual.table <- xtable(
  final.qual,
  caption = c('short caption', 'full caption'),
  label = 'tab:qual_scores',
  booktabs = TRUE
)

print(export.qual.table, size="\\tiny", file = '../manuscript/tables/qual_scores.tex')
```

